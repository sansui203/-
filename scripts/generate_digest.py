#!/usr/bin/env python3
"""
AI èµ„è®¯èšåˆå™¨ - çƒ­åŠ è½½ç‰ˆæœ¬
æ¯ä¸ªæ•°æ®æºç‹¬ç«‹è¿è¡Œï¼Œå¤±è´¥ä¸å½±å“å…¶ä»–
"""

import os
import json
import requests
import feedparser
from datetime import datetime, timedelta
from pathlib import Path


class AIDigestGenerator:
    def __init__(self):
        self.siliconflow_key = os.environ.get("SILICONFLOW_API_KEY")
        self.youtube_key = os.environ.get("YOUTUBE_API_KEY")
        self.twitter_key = os.environ.get("TWITTER_API_KEY")
        self.rapidapi_key = os.environ.get("RAPIDAPI_KEY")
        
        self.model = os.environ.get("SILICONFLOW_MODEL", "deepseek-ai/DeepSeek-V3")
        
        self.today = datetime.now()
        self.today_str = self.today.strftime("%Y-%m-%d")
        self.yesterday = self.today - timedelta(days=1)
        
        self.data_dir = Path("data")
        self.data_dir.mkdir(exist_ok=True)
        
        self.all_items = []
        
        # æ‰“å° API çŠ¶æ€
        print("ğŸ“‹ API çŠ¶æ€:")
        print(f"  - ç¡…åŸºæµåŠ¨: {'âœ…' if self.siliconflow_key else 'âŒ æœªé…ç½®'}")
        print(f"  - YouTube: {'âœ…' if self.youtube_key else 'âš ï¸ è·³è¿‡'}")
        print(f"  - Twitter: {'âœ…' if self.twitter_key else 'âš ï¸ è·³è¿‡'}")
        print(f"  - TikTok: {'âœ…' if self.rapidapi_key else 'âš ï¸ è·³è¿‡'}")

    def safe_fetch(self, name, func):
        """å®‰å…¨æ‰§è¡Œæ•°æ®è·å–ï¼Œå¤±è´¥ä¸å½±å“å…¶ä»–"""
        try:
            func()
        except Exception as e:
            print(f"  âŒ {name} å¤±è´¥: {e}")

    # ==================== RSSï¼ˆæ— éœ€ APIï¼‰====================
    
    def fetch_rss(self):
        """è·å– RSS æ–°é—»"""
        print("\nğŸ“° RSS æ–°é—»...")
        
        sources = [
            ("https://www.nytimes.com/svc/collections/v1/publish/https://www.nytimes.com/spotlight/artificial-intelligence/rss.xml", "çº½çº¦æ—¶æŠ¥"),
            ("https://techcrunch.com/category/artificial-intelligence/feed/", "TechCrunch"),
            ("https://www.theverge.com/rss/ai-artificial-intelligence/index.xml", "The Verge"),
        ]
        
        for url, name in sources:
            try:
                feed = feedparser.parse(url)
                count = 0
                for entry in feed.entries[:10]:
                    pub = entry.get("published_parsed") or entry.get("updated_parsed")
                    if pub:
                        dt = datetime(*pub[:6])
                        if dt > self.yesterday:
                            self.all_items.append({
                                "æ ‡é¢˜": entry.get("title", ""),
                                "å†…å®¹": entry.get("summary", "")[:200],
                                "æ—¥æœŸ": dt.isoformat(),
                                "æ¥æº": name,
                                "æ¿å—": "æ–°é—»",
                                "é“¾æ¥": entry.get("link", "")
                            })
                            count += 1
                print(f"  âœ… {name}: {count} æ¡")
            except Exception as e:
                print(f"  âŒ {name}: {e}")

    # ==================== YouTube åšä¸»ï¼ˆRSSï¼Œæ— éœ€ APIï¼‰====================
    
    def fetch_youtube_rss(self):
        """è·å– YouTube åšä¸»æ›´æ–°"""
        print("\nğŸ“º YouTube åšä¸»...")
        
        channels = [
            "UCNJ1Ymd5yFuUPtn21xtRbbw",
            "UChpleBmo18P08aKCIgti38g", 
            "UCPjNBjflYl0-HQtUvOx0Ibw"
        ]
        
        for cid in channels:
            try:
                feed = feedparser.parse(f"https://www.youtube.com/feeds/videos.xml?channel_id={cid}")
                name = feed.feed.get("author", "YouTube")
                count = 0
                for entry in feed.entries[:3]:
                    pub = entry.get("published_parsed")
                    if pub:
                        dt = datetime(*pub[:6])
                        if dt > self.yesterday:
                            self.all_items.append({
                                "æ ‡é¢˜": entry.get("title", ""),
                                "å†…å®¹": "",
                                "æ—¥æœŸ": dt.isoformat(),
                                "æ¥æº": name,
                                "æ¿å—": "æ²¹ç®¡åšä¸»",
                                "é“¾æ¥": entry.get("link", "")
                            })
                            count += 1
                print(f"  âœ… {name}: {count} æ¡")
            except Exception as e:
                print(f"  âŒ é¢‘é“ {cid[:8]}: {e}")

    # ==================== YouTube çƒ­é—¨ï¼ˆéœ€è¦ APIï¼‰====================
    
    def fetch_youtube_trending(self):
        """è·å– YouTube çƒ­é—¨è§†é¢‘"""
        if not self.youtube_key:
            return
        
        print("\nğŸ”¥ YouTube çƒ­é—¨...")
        
        try:
            # æœç´¢
            r = requests.get("https://www.googleapis.com/youtube/v3/search", params={
                "key": self.youtube_key,
                "part": "snippet",
                "q": "AI",
                "order": "relevance",
                "maxResults": 10,
                "regionCode": "US",
                "type": "video",
                "publishedAfter": self.yesterday.isoformat() + "Z"
            }, timeout=30)
            data = r.json()
            
            if "items" not in data:
                print(f"  âŒ {data.get('error', {}).get('message', 'é”™è¯¯')}")
                return
            
            ids = [i["id"]["videoId"] for i in data["items"]]
            
            # ç»Ÿè®¡
            r2 = requests.get("https://www.googleapis.com/youtube/v3/videos", params={
                "key": self.youtube_key,
                "part": "statistics",
                "id": ",".join(ids)
            }, timeout=30)
            stats = {i["id"]: i["statistics"] for i in r2.json().get("items", [])}
            
            count = 0
            for item in data["items"]:
                vid = item["id"]["videoId"]
                views = int(stats.get(vid, {}).get("viewCount", 0))
                if views > 200000:
                    self.all_items.append({
                        "æ ‡é¢˜": item["snippet"]["title"],
                        "å†…å®¹": item["snippet"]["description"][:150],
                        "æ—¥æœŸ": item["snippet"]["publishTime"],
                        "æ¥æº": "YouTube",
                        "æ¿å—": "YouTubeçƒ­ç‚¹",
                        "é“¾æ¥": f"https://youtube.com/watch?v={vid}"
                    })
                    count += 1
            print(f"  âœ… {count} æ¡ (æ’­æ”¾é‡>20ä¸‡)")
        except Exception as e:
            print(f"  âŒ {e}")

    # ==================== Twitterï¼ˆéœ€è¦ APIï¼‰====================
    
    def fetch_twitter(self):
        """è·å– Twitter çƒ­é—¨"""
        if not self.twitter_key:
            return
        
        print("\nğŸ¦ Twitter çƒ­é—¨...")
        
        try:
            r = requests.get("https://api.twitterapi.io/twitter/tweet/advanced_search",
                headers={"x-api-key": self.twitter_key},
                params={"query": "AI", "queryType": "Top"},
                timeout=30)
            data = r.json()
            
            count = 0
            for t in data.get("tweets", []):
                views = t.get("viewCount", 0)
                heat = t.get("likeCount", 0) + t.get("retweetCount", 0) * 2
                if views > 10000 and heat > 1000:
                    self.all_items.append({
                        "æ ‡é¢˜": t.get("text", "")[:100],
                        "å†…å®¹": t.get("text", ""),
                        "æ—¥æœŸ": t.get("createdAt", ""),
                        "æ¥æº": "Twitter",
                        "æ¿å—": "Twitterçƒ­ç‚¹",
                        "é“¾æ¥": t.get("url", "")
                    })
                    count += 1
            print(f"  âœ… {count} æ¡")
        except Exception as e:
            print(f"  âŒ {e}")

    def fetch_twitter_accounts(self):
        """è·å–æ˜æ˜Ÿå…¬å¸åŠ¨æ€"""
        if not self.twitter_key:
            return
        
        print("\nğŸŒŸ æ˜æ˜Ÿå…¬å¸åŠ¨æ€...")
        
        for user in ["OpenAI", "GoogleDeepMind", "GoogleAIStudio"]:
            try:
                r = requests.get("https://api.twitterapi.io/twitter/user/last_tweets",
                    headers={"x-api-key": self.twitter_key},
                    params={"userName": user},
                    timeout=30)
                data = r.json()
                
                count = 0
                for t in data.get("data", {}).get("tweets", [])[:5]:
                    text = t.get("text", "")
                    if t.get("retweeted_tweet"):
                        text = f"(è½¬å‘) {t['retweeted_tweet'].get('text', '')}"
                    self.all_items.append({
                        "æ ‡é¢˜": text[:100],
                        "å†…å®¹": text,
                        "æ—¥æœŸ": t.get("createdAt", ""),
                        "æ¥æº": user,
                        "æ¿å—": "æ˜æ˜Ÿå…¬å¸åŠ¨æ€",
                        "é“¾æ¥": t.get("url", "")
                    })
                    count += 1
                print(f"  âœ… @{user}: {count} æ¡")
            except Exception as e:
                print(f"  âŒ @{user}: {e}")

    # ==================== TikTokï¼ˆéœ€è¦ APIï¼‰====================
    
    def fetch_tiktok(self):
        """è·å– TikTok çƒ­é—¨"""
        if not self.rapidapi_key:
            return
        
        print("\nğŸµ TikTok çƒ­é—¨...")
        
        try:
            r = requests.get("https://tiktok-api23.p.rapidapi.com/api/search/general",
                headers={
                    "x-rapidapi-key": self.rapidapi_key,
                    "x-rapidapi-host": "tiktok-api23.p.rapidapi.com"
                },
                params={"keyword": "AI", "cursor": "0"},
                timeout=30)
            data = r.json()
            
            count = 0
            for item in data.get("data", []):
                d = item.get("item", {})
                plays = d.get("stats", {}).get("playCount", 0)
                followers = d.get("authorStats", {}).get("followerCount", 1)
                
                if plays > 100000 and plays / followers > 3:
                    ts = d.get("createTime", 0)
                    if ts and datetime.fromtimestamp(ts) > self.today - timedelta(days=14):
                        author = d.get("author", {})
                        self.all_items.append({
                            "æ ‡é¢˜": d.get("desc", "")[:100],
                            "å†…å®¹": d.get("desc", ""),
                            "æ—¥æœŸ": datetime.fromtimestamp(ts).isoformat(),
                            "æ¥æº": "TikTok",
                            "æ¿å—": "TikTokçƒ­ç‚¹",
                            "é“¾æ¥": f"https://tiktok.com/@{author.get('uniqueId', '')}/video/{d.get('id', '')}"
                        })
                        count += 1
            print(f"  âœ… {count} æ¡")
        except Exception as e:
            print(f"  âŒ {e}")

    # ==================== GitHubï¼ˆæ— éœ€ APIï¼‰====================
    
    def fetch_github_trending(self):
        """è·å– GitHub Trendingï¼ˆå¤šä¸ªå¤‡ç”¨æ–¹æ¡ˆï¼‰"""
        print("\nâ­ GitHub Trending...")
        
        periods = [
            ("daily", "ä»Šæ—¥çƒ­é—¨"),
            ("weekly", "æœ¬å‘¨çƒ­é—¨")
        ]
        
        for period, label in periods:
            count = 0
            
            # æ–¹æ¡ˆ1: å°è¯•å¤šä¸ª Trending API
            # GitHub Search API å¤‡ç”¨æ–¹æ¡ˆï¼šæŸ¥è¯¢æœ€è¿‘æ´»è·ƒçš„é«˜æ˜Ÿé¡¹ç›®
            if period == "daily":
                # ä»Šæ—¥ï¼šæœ€è¿‘ 1 å¤©æ›´æ–°çš„é¡¹ç›®ï¼Œstar > 1000
                date_range = self.yesterday.strftime('%Y-%m-%d')
                stars_req = "1000"
                search_field = "pushed"  # ä½¿ç”¨ pushedï¼ˆæœ€è¿‘æ›´æ–°ï¼‰è€Œä¸æ˜¯ created
            else:
                # æœ¬å‘¨ï¼šæœ€è¿‘ 7 å¤©æ›´æ–°çš„é¡¹ç›®ï¼Œstar > 500
                date_range = (self.today - timedelta(days=7)).strftime('%Y-%m-%d')
                stars_req = "500"
                search_field = "pushed"
            
            apis = [
                f"https://api.gitterapp.com/repositories?since={period}",
                f"https://gh-trending-api.herokuapp.com/repositories?since={period}",
                f"https://api.github.com/search/repositories?q=stars:>{stars_req}+{search_field}:>{date_range}&sort=stars&order=desc&per_page=8",
            ]
            
            for api_url in apis:
                try:
                    headers = {"User-Agent": "Mozilla/5.0"}
                    r = requests.get(api_url, headers=headers, timeout=30)
                    if r.status_code != 200:
                        print(f"      âš ï¸ {api_url[:50]}... -> HTTP {r.status_code}")
                        continue
                    
                    data = r.json()
                    
                    # GitHub Search API è¿”å›æ ¼å¼ä¸åŒ
                    if "items" in data:
                        repos = data["items"]
                    else:
                        repos = data if isinstance(data, list) else []
                    
                    if not repos:
                        print(f"      âš ï¸ {api_url[:50]}... -> è¿”å›ç©ºæ•°æ®")
                        continue
                    
                    for repo in repos[:8]:
                        # å…¼å®¹å¤šç§ API è¿”å›æ ¼å¼
                        if "full_name" in repo:  # GitHub Search API
                            author, name = repo["full_name"].split("/") if "/" in repo["full_name"] else ("", repo["full_name"])
                        else:  # Trending API
                            author = repo.get("author", "") or repo.get("username", "")
                            name = repo.get("name", "") or repo.get("reponame", "")
                        
                        if not author or not name:
                            continue
                            
                        desc = repo.get("description", "") or ""
                        lang = repo.get("language", "") or repo.get("programmingLanguage", "") or "Unknown"
                        
                        # æ˜Ÿæ ‡æ•°
                        stars = (
                            repo.get("stars") or 
                            repo.get("totalStars") or 
                            repo.get("stargazers_count") or 
                            0
                        )
                        stars_today = repo.get("starsSince", 0) or repo.get("starsToday", 0)
                        
                        self.all_items.append({
                            "æ ‡é¢˜": f"{author}/{name}",
                            "å†…å®¹": desc[:200] if desc else f"{lang} é¡¹ç›®",
                            "æ—¥æœŸ": self.today.isoformat(),
                            "æ¥æº": f"GitHub {label}",
                            "æ¿å—": f"GitHub{label}",
                            "é“¾æ¥": repo.get("url") or repo.get("html_url") or f"https://github.com/{author}/{name}",
                            "é¢å¤–": f"â­ {stars:,} | ğŸ”¥ +{stars_today:,} | ğŸ’» {lang}" if stars_today else f"â­ {stars:,} | ğŸ’» {lang}"
                        })
                        count += 1
                    
                    if count > 0:
                        print(f"  âœ… {label}: {count} æ¡ï¼ˆä½¿ç”¨ {api_url.split('/')[2]}ï¼‰")
                        break  # æˆåŠŸå°±ä¸å°è¯•ä¸‹ä¸€ä¸ª API
                        
                except Exception as e:
                    print(f"      âŒ {api_url[:50]}... -> {type(e).__name__}: {str(e)[:100]}")
                    continue
            
            if count == 0:
                print(f"  âš ï¸ {label}: æ‰€æœ‰ API å‡å¤±è´¥")

    # ==================== HuggingFaceï¼ˆæ— éœ€ APIï¼‰====================
    
    def fetch_huggingface_trending(self):
        """è·å– HuggingFace çƒ­é—¨æ¨¡å‹ï¼ˆå·²éªŒè¯å¯ç”¨ï¼‰"""
        print("\nğŸ¤— HuggingFace Trending...")
        
        try:
            # ä½¿ç”¨ HuggingFace å®˜æ–¹ APIï¼ˆå®æµ‹å¯ç”¨ï¼‰
            r = requests.get(
                "https://huggingface.co/api/models",
                params={"limit": 10},  # æŒ‰ trendingScore é»˜è®¤æ’åº
                headers={"User-Agent": "Mozilla/5.0"},
                timeout=30
            )
            
            if r.status_code != 200:
                print(f"  âŒ HTTP {r.status_code}: {r.text[:200]}")
                return
                
            models = r.json()
            if not isinstance(models, list):
                print(f"  âŒ è¿”å›æ ¼å¼é”™è¯¯")
                return
            
            count = 0
            for model in models[:8]:
                if not isinstance(model, dict):
                    continue
                    
                model_id = model.get("id", "")
                if not model_id:
                    continue
                    
                downloads = model.get("downloads", 0) or 0
                likes = model.get("likes", 0) or 0
                trending = model.get("trendingScore", 0) or 0
                
                # è·å–æ ‡ç­¾å’Œä»»åŠ¡ç±»å‹
                tags = model.get("tags", [])
                task = next((t for t in tags if not t.startswith(("license:", "region:", "arxiv:"))), "æ¨¡å‹")
                
                self.all_items.append({
                    "æ ‡é¢˜": model_id,
                    "å†…å®¹": f"{task} | çƒ­åº¦: {trending}",
                    "æ—¥æœŸ": self.today.isoformat(),
                    "æ¥æº": "HuggingFace",
                    "æ¿å—": "HuggingFaceçƒ­é—¨",
                    "é“¾æ¥": f"https://huggingface.co/{model_id}",
                    "é¢å¤–": f"ğŸ“¥ {downloads:,} ä¸‹è½½ | â¤ï¸ {likes} ç‚¹èµ | ğŸ”¥ çƒ­åº¦ {trending}"
                })
                count += 1
            
            print(f"  âœ… {count} æ¡")
        except Exception as e:
            print(f"  âŒ {type(e).__name__}: {str(e)[:100]}")
    
    # ==================== ModelScopeï¼ˆæ— éœ€ APIï¼‰====================
    
    def fetch_modelscope_trending(self):
        """è·å– ModelScope çƒ­é—¨æ¨¡å‹ï¼ˆAPI å·²éªŒè¯å¤±æ•ˆï¼Œæš‚æ—¶è·³è¿‡ï¼‰"""
        print("\nğŸ”® ModelScope Trending...")
        print("  âš ï¸ ModelScope API å·²åºŸå¼ƒï¼ˆå®æµ‹ 404ï¼‰ï¼Œè·³è¿‡æ­¤æ•°æ®æº")
        # æ³¨ï¼šç»å®æµ‹ https://modelscope.cn/api/v1/models è¿”å› 404
        # ModelScope å¯èƒ½éœ€è¦è®¤è¯æˆ– API å·²è¿ç§»
        return

    def _fetch_modelscope_old(self):
        """æ—§çš„ ModelScope è·å–ä»£ç ï¼ˆå·²åºŸå¼ƒï¼Œä¿ç•™ä½œå‚è€ƒï¼‰"""
        for url, params in [("https://modelscope.cn/api/v1/models", {"PageSize": 10})]:
            try:
                r = requests.get(
                    url, 
                    params=params, 
                    headers={
                        "User-Agent": "Mozilla/5.0",
                        "Referer": "https://modelscope.cn/"
                    },
                    timeout=30
                )
                
                if r.status_code != 200:
                    print(f"      âš ï¸ {url.split('/')[2]} -> HTTP {r.status_code}")
                    continue
                
                data = r.json()
                
                # å°è¯•å¤šç§æ•°æ®ç»“æ„
                models_data = (
                    data.get("Data") or 
                    data.get("data") or 
                    data.get("models") or 
                    []
                )
                
                if not models_data or not isinstance(models_data, list):
                    print(f"      âš ï¸ {url.split('/')[2]} -> è¿”å›æ•°æ®æ ¼å¼é”™è¯¯æˆ–ä¸ºç©º")
                    continue
                
                count = 0
                for model in models_data[:8]:
                    if not isinstance(model, dict):
                        continue
                    
                    # å¤šç§å­—æ®µåå°è¯•
                    model_name = (
                        model.get("Path") or 
                        model.get("Name") or 
                        model.get("Id") or 
                        model.get("ModelId") or
                        ""
                    )
                    
                    if not model_name:
                        continue
                    
                    desc = model.get("ChineseDescription") or model.get("Description", "")
                    if desc and isinstance(desc, str):
                        desc = desc[:150]
                    else:
                        desc = "ModelScope çƒ­é—¨æ¨¡å‹"
                    
                    downloads = model.get("Downloads", 0) or model.get("DownloadCount", 0) or 0
                    
                    self.all_items.append({
                        "æ ‡é¢˜": model_name,
                        "å†…å®¹": desc,
                        "æ—¥æœŸ": self.today.isoformat(),
                        "æ¥æº": "ModelScope",
                        "æ¿å—": "ModelScopeçƒ­é—¨",
                        "é“¾æ¥": f"https://modelscope.cn/models/{model_name}",
                        "é¢å¤–": f"ğŸ“¥ {downloads:,} ä¸‹è½½"
                    })
                    count += 1
                
                if count > 0:
                    print(f"  âœ… {count} æ¡ï¼ˆä½¿ç”¨ {url.split('/')[2]}ï¼‰")
                    return  # æˆåŠŸå°±é€€å‡º
                    
            except Exception as e:
                print(f"      âŒ {url.split('/')[2]} -> {type(e).__name__}: {str(e)[:100]}")
                continue
        
        print("  âš ï¸ æ‰€æœ‰æ¥å£å‡å¤±è´¥ï¼ˆModelScope å¯èƒ½éœ€è¦ç™»å½•æˆ–åœ¨å›½å¤–è®¿é—®å—é™ï¼‰")

    # ==================== AI å¤„ç† ====================
    
    def ai_process(self):
        """AI ç¿»è¯‘å’Œæ‘˜è¦"""
        if not self.siliconflow_key:
            error_msg = "âŒ æœªé…ç½® SILICONFLOW_API_KEYï¼Œæ— æ³•è¿›è¡Œ AI å¤„ç†"
            print(f"\n{error_msg}")
            
            # ä¿å­˜é”™è¯¯ä¿¡æ¯ï¼ˆé™åˆ¶5æ¡ï¼‰
            fallback = {
                "date": self.today_str,
                "error": error_msg,
                "categories": {"åŸå§‹æ•°æ®": self.all_items[:5]},
                "analysis": {
                    "summary": "âš ï¸ æœªé…ç½® API Keyï¼Œè¯·åœ¨ GitHub Secrets ä¸­æ·»åŠ  SILICONFLOW_API_KEYï¼ˆæ˜¾ç¤ºå‰5æ¡ï¼‰",
                    "trends": []
                }
            }
            (self.data_dir / "latest.json").write_text(
                json.dumps(fallback, ensure_ascii=False, indent=2), encoding="utf-8")
            return fallback
        
        if not self.all_items:
            print("\nâš ï¸ æ²¡æœ‰æ•°æ®")
            return None
        
        print(f"\nğŸ¤– AI å¤„ç† ({self.model})...")
        
        prompt = f"""You are a JSON formatter. Process the following AI news data and return ONLY valid JSON, no extra text.

Input data:
{json.dumps(self.all_items[:100], ensure_ascii=False)}

Requirements:
1. Translate English to Chinese
2. Summarize long content to 60-80 Chinese characters
3. Group by category
4. Keep "é¢å¤–" field (stars, downloads, etc.)
5. **IMPORTANT: Each category should have AT MOST 5 items (select the most important/popular ones)**

Output format (ONLY this JSON, nothing else):
{{"date":"{self.today_str}","categories":{{"æ–°é—»":[],"æ˜æ˜Ÿå…¬å¸åŠ¨æ€":[],"æ²¹ç®¡åšä¸»":[],"YouTubeçƒ­ç‚¹":[],"Twitterçƒ­ç‚¹":[],"TikTokçƒ­ç‚¹":[],"GitHubä»Šæ—¥çƒ­é—¨":[],"GitHubæœ¬å‘¨çƒ­é—¨":[],"HuggingFaceçƒ­é—¨":[]}},"analysis":{{"summary":"ä»Šæ—¥æ‘˜è¦","trends":["è¶‹åŠ¿1","è¶‹åŠ¿2"]}}}}

CRITICAL: Return ONLY the JSON object, no markdown, no code blocks, no explanations. Maximum 5 items per category."""

        try:
            from openai import OpenAI
            client = OpenAI(
                api_key=self.siliconflow_key,
                base_url="https://api.siliconflow.cn/v1"
            )
            
            resp = client.chat.completions.create(
                model=self.model,
                messages=[
                    {"role": "system", "content": "You are a JSON formatter. Always return valid JSON only, no extra text."},
                    {"role": "user", "content": prompt}
                ],
                max_tokens=8000,
                temperature=0.1  # é™ä½æ¸©åº¦ä½¿è¾“å‡ºæ›´ç¨³å®š
            )
            
            content = resp.choices[0].message.content.strip()
            
            # å¤šç§æ–¹å¼æå– JSON
            result = None
            errors = []
            
            # æ–¹æ³•1: ç›´æ¥è§£æ
            try:
                result = json.loads(content)
            except Exception as e1:
                errors.append(f"ç›´æ¥è§£æå¤±è´¥: {e1}")
                
                # æ–¹æ³•2: ç§»é™¤ markdown ä»£ç å—
                try:
                    if "```" in content:
                        content = content.split("```")[1]
                        content = content.replace("json", "").replace("JSON", "").strip()
                    result = json.loads(content)
                except Exception as e2:
                    errors.append(f"ç§»é™¤ä»£ç å—åå¤±è´¥: {e2}")
                    
                    # æ–¹æ³•3: æå–ç¬¬ä¸€ä¸ª { åˆ°æœ€åä¸€ä¸ª }
                    try:
                        start = content.find("{")
                        end = content.rfind("}") + 1
                        if start >= 0 and end > start:
                            content = content[start:end]
                        result = json.loads(content)
                    except Exception as e3:
                        errors.append(f"æå–æ‹¬å·åå¤±è´¥: {e3}")
                        
                        # ä¿å­˜åŸå§‹å†…å®¹ä»¥ä¾¿è°ƒè¯•
                        debug_file = self.data_dir / f"debug_response_{self.today_str}.txt"
                        debug_file.write_text(f"åŸå§‹è¿”å›:\n{resp.choices[0].message.content}\n\né”™è¯¯:\n" + "\n".join(errors), encoding="utf-8")
                        raise Exception(f"æ‰€æœ‰JSONè§£ææ–¹æ³•å‡å¤±è´¥ã€‚è¯¦è§ {debug_file}")
            
            if not result:
                raise Exception("æ— æ³•è§£æ AI è¿”å›çš„ JSON")
            
            # ç¡®ä¿æ¯ä¸ªåˆ†ç±»æœ€å¤š5æ¡
            categories = result.get("categories", {})
            for category_name, items in categories.items():
                if isinstance(items, list) and len(items) > 5:
                    categories[category_name] = items[:5]
            
            # ä¿å­˜
            (self.data_dir / f"digest_{self.today_str}.json").write_text(
                json.dumps(result, ensure_ascii=False, indent=2), encoding="utf-8")
            (self.data_dir / "latest.json").write_text(
                json.dumps(result, ensure_ascii=False, indent=2), encoding="utf-8")
            
            total = sum(len(v) for v in result.get("categories", {}).values())
            print(f"  âœ… å®Œæˆï¼Œå…± {total} æ¡ï¼ˆæ¯åˆ†ç±»æœ€å¤š5æ¡ï¼‰")
            return result
            
        except Exception as e:
            import traceback
            error_msg = f"AI å¤„ç†å¤±è´¥: {str(e)}\n{traceback.format_exc()}"
            print(f"  âŒ {error_msg}")
            
            # å¤±è´¥æ—¶ä¿å­˜åŸå§‹æ•°æ®ï¼Œå¹¶é™„å¸¦é”™è¯¯ä¿¡æ¯ï¼ˆé™åˆ¶5æ¡ï¼‰
            fallback = {
                "date": self.today_str,
                "error": error_msg,
                "categories": {"åŸå§‹æ•°æ®": self.all_items[:5]},
                "analysis": {
                    "summary": f"âš ï¸ AI å¤„ç†å¤±è´¥ï¼Œæ˜¾ç¤ºåŸå§‹æ•°æ®ï¼ˆå‰5æ¡ï¼‰ã€‚é”™è¯¯ï¼š{str(e)}",
                    "trends": []
                }
            }
            (self.data_dir / "latest.json").write_text(
                json.dumps(fallback, ensure_ascii=False, indent=2), encoding="utf-8")
            return fallback

    def run(self):
        print("=" * 50)
        print(f"ğŸš€ AI èµ„è®¯èšåˆå™¨ - {self.today_str}")
        print("=" * 50)
        
        # æ•°æ®é‡‡é›†ï¼ˆæ¯ä¸ªç‹¬ç«‹ï¼Œå¤±è´¥ä¸å½±å“å…¶ä»–ï¼‰
        self.safe_fetch("RSS", self.fetch_rss)
        self.safe_fetch("YouTubeåšä¸»", self.fetch_youtube_rss)
        self.safe_fetch("YouTubeçƒ­é—¨", self.fetch_youtube_trending)
        self.safe_fetch("Twitterçƒ­é—¨", self.fetch_twitter)
        self.safe_fetch("Twitterè´¦å·", self.fetch_twitter_accounts)
        self.safe_fetch("TikTok", self.fetch_tiktok)
        self.safe_fetch("GitHubçƒ­é—¨", self.fetch_github_trending)
        self.safe_fetch("HuggingFace", self.fetch_huggingface_trending)
        self.safe_fetch("ModelScope", self.fetch_modelscope_trending)
        
        print(f"\nğŸ“¦ å…±é‡‡é›† {len(self.all_items)} æ¡")
        
        # AI å¤„ç†
        result = self.ai_process()
        
        print("\n" + "=" * 50)
        print("âœ¨ å®Œæˆ!")
        return result


if __name__ == "__main__":
    AIDigestGenerator().run()
