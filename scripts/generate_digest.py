#!/usr/bin/env python3
"""
AI èµ„è®¯èšåˆå™¨ - çƒ­åŠ è½½ç‰ˆæœ¬
æ¯ä¸ªæ•°æ®æºç‹¬ç«‹è¿è¡Œï¼Œå¤±è´¥ä¸å½±å“å…¶ä»–
"""

import os
import json
import requests
import feedparser
from datetime import datetime, timedelta
from pathlib import Path


class AIDigestGenerator:
    def __init__(self):
        self.siliconflow_key = os.environ.get("SILICONFLOW_API_KEY")
        self.youtube_key = os.environ.get("YOUTUBE_API_KEY")
        self.twitter_key = os.environ.get("TWITTER_API_KEY")
        self.rapidapi_key = os.environ.get("RAPIDAPI_KEY")
        
        self.model = os.environ.get("SILICONFLOW_MODEL", "deepseek-ai/DeepSeek-V3")
        
        self.today = datetime.now()
        self.today_str = self.today.strftime("%Y-%m-%d")
        self.yesterday = self.today - timedelta(days=1)
        
        self.data_dir = Path("data")
        self.data_dir.mkdir(exist_ok=True)
        
        self.all_items = []
        
        # æ‰“å° API çŠ¶æ€
        print("ğŸ“‹ API çŠ¶æ€:")
        print(f"  - ç¡…åŸºæµåŠ¨: {'âœ…' if self.siliconflow_key else 'âŒ æœªé…ç½®'}")
        print(f"  - YouTube: {'âœ…' if self.youtube_key else 'âš ï¸ è·³è¿‡'}")
        print(f"  - Twitter: {'âœ…' if self.twitter_key else 'âš ï¸ è·³è¿‡'}")
        print(f"  - TikTok: {'âœ…' if self.rapidapi_key else 'âš ï¸ è·³è¿‡'}")

    def safe_fetch(self, name, func):
        """å®‰å…¨æ‰§è¡Œæ•°æ®è·å–ï¼Œå¤±è´¥ä¸å½±å“å…¶ä»–"""
        try:
            func()
        except Exception as e:
            print(f"  âŒ {name} å¤±è´¥: {e}")

    # ==================== RSSï¼ˆæ— éœ€ APIï¼‰====================
    
    def fetch_rss(self):
        """è·å– RSS æ–°é—»"""
        print("\nğŸ“° RSS æ–°é—»...")
        
        sources = [
            ("https://www.nytimes.com/svc/collections/v1/publish/https://www.nytimes.com/spotlight/artificial-intelligence/rss.xml", "çº½çº¦æ—¶æŠ¥"),
            ("https://techcrunch.com/category/artificial-intelligence/feed/", "TechCrunch"),
            ("https://www.theverge.com/rss/ai-artificial-intelligence/index.xml", "The Verge"),
        ]
        
        for url, name in sources:
            try:
                feed = feedparser.parse(url)
                count = 0
                for entry in feed.entries[:10]:
                    pub = entry.get("published_parsed") or entry.get("updated_parsed")
                    if pub:
                        dt = datetime(*pub[:6])
                        if dt > self.yesterday:
                            self.all_items.append({
                                "æ ‡é¢˜": entry.get("title", ""),
                                "å†…å®¹": entry.get("summary", "")[:200],
                                "æ—¥æœŸ": dt.isoformat(),
                                "æ¥æº": name,
                                "æ¿å—": "æ–°é—»",
                                "é“¾æ¥": entry.get("link", "")
                            })
                            count += 1
                print(f"  âœ… {name}: {count} æ¡")
            except Exception as e:
                print(f"  âŒ {name}: {e}")

    # ==================== YouTube åšä¸»ï¼ˆRSSï¼Œæ— éœ€ APIï¼‰====================
    
    def fetch_youtube_rss(self):
        """è·å– YouTube åšä¸»æ›´æ–°"""
        print("\nğŸ“º YouTube åšä¸»...")
        
        channels = [
            "UCNJ1Ymd5yFuUPtn21xtRbbw",
            "UChpleBmo18P08aKCIgti38g", 
            "UCPjNBjflYl0-HQtUvOx0Ibw"
        ]
        
        for cid in channels:
            try:
                feed = feedparser.parse(f"https://www.youtube.com/feeds/videos.xml?channel_id={cid}")
                name = feed.feed.get("author", "YouTube")
                count = 0
                for entry in feed.entries[:3]:
                    pub = entry.get("published_parsed")
                    if pub:
                        dt = datetime(*pub[:6])
                        if dt > self.yesterday:
                            self.all_items.append({
                                "æ ‡é¢˜": entry.get("title", ""),
                                "å†…å®¹": "",
                                "æ—¥æœŸ": dt.isoformat(),
                                "æ¥æº": name,
                                "æ¿å—": "æ²¹ç®¡åšä¸»",
                                "é“¾æ¥": entry.get("link", "")
                            })
                            count += 1
                print(f"  âœ… {name}: {count} æ¡")
            except Exception as e:
                print(f"  âŒ é¢‘é“ {cid[:8]}: {e}")

    # ==================== YouTube çƒ­é—¨ï¼ˆéœ€è¦ APIï¼‰====================
    
    def fetch_youtube_trending(self):
        """è·å– YouTube çƒ­é—¨è§†é¢‘"""
        if not self.youtube_key:
            return
        
        print("\nğŸ”¥ YouTube çƒ­é—¨...")
        
        try:
            # æœç´¢
            r = requests.get("https://www.googleapis.com/youtube/v3/search", params={
                "key": self.youtube_key,
                "part": "snippet",
                "q": "AI",
                "order": "relevance",
                "maxResults": 50,
                "regionCode": "US",
                "type": "video",
                "publishedAfter": self.yesterday.isoformat() + "Z"
            }, timeout=30)
            data = r.json()
            
            if "items" not in data:
                print(f"  âŒ {data.get('error', {}).get('message', 'é”™è¯¯')}")
                return
            
            ids = [i["id"]["videoId"] for i in data["items"]]
            
            # ç»Ÿè®¡
            r2 = requests.get("https://www.googleapis.com/youtube/v3/videos", params={
                "key": self.youtube_key,
                "part": "statistics",
                "id": ",".join(ids)
            }, timeout=30)
            stats = {i["id"]: i["statistics"] for i in r2.json().get("items", [])}
            
            count = 0
            for item in data["items"]:
                vid = item["id"]["videoId"]
                views = int(stats.get(vid, {}).get("viewCount", 0))
                if views > 200000:
                    self.all_items.append({
                        "æ ‡é¢˜": item["snippet"]["title"],
                        "å†…å®¹": item["snippet"]["description"][:150],
                        "æ—¥æœŸ": item["snippet"]["publishTime"],
                        "æ¥æº": "YouTube",
                        "æ¿å—": "YouTubeçƒ­ç‚¹",
                        "é“¾æ¥": f"https://youtube.com/watch?v={vid}"
                    })
                    count += 1
            print(f"  âœ… {count} æ¡ (æ’­æ”¾é‡>20ä¸‡)")
        except Exception as e:
            print(f"  âŒ {e}")

    # ==================== Twitterï¼ˆéœ€è¦ APIï¼‰====================
    
    def fetch_twitter(self):
        """è·å– Twitter çƒ­é—¨"""
        if not self.twitter_key:
            return
        
        print("\nğŸ¦ Twitter çƒ­é—¨...")
        
        try:
            r = requests.get("https://api.twitterapi.io/twitter/tweet/advanced_search",
                headers={"x-api-key": self.twitter_key},
                params={"query": "AI", "queryType": "Top"},
                timeout=30)
            data = r.json()
            
            count = 0
            for t in data.get("tweets", []):
                views = t.get("viewCount", 0)
                heat = t.get("likeCount", 0) + t.get("retweetCount", 0) * 2
                if views > 10000 and heat > 1000:
                    self.all_items.append({
                        "æ ‡é¢˜": t.get("text", "")[:100],
                        "å†…å®¹": t.get("text", ""),
                        "æ—¥æœŸ": t.get("createdAt", ""),
                        "æ¥æº": "Twitter",
                        "æ¿å—": "Twitterçƒ­ç‚¹",
                        "é“¾æ¥": t.get("url", "")
                    })
                    count += 1
            print(f"  âœ… {count} æ¡")
        except Exception as e:
            print(f"  âŒ {e}")

    def fetch_twitter_accounts(self):
        """è·å–æ˜æ˜Ÿå…¬å¸åŠ¨æ€"""
        if not self.twitter_key:
            return
        
        print("\nğŸŒŸ æ˜æ˜Ÿå…¬å¸åŠ¨æ€...")
        
        for user in ["OpenAI", "GoogleDeepMind", "GoogleAIStudio"]:
            try:
                r = requests.get("https://api.twitterapi.io/twitter/user/last_tweets",
                    headers={"x-api-key": self.twitter_key},
                    params={"userName": user},
                    timeout=30)
                data = r.json()
                
                count = 0
                for t in data.get("data", {}).get("tweets", [])[:5]:
                    text = t.get("text", "")
                    if t.get("retweeted_tweet"):
                        text = f"(è½¬å‘) {t['retweeted_tweet'].get('text', '')}"
                    self.all_items.append({
                        "æ ‡é¢˜": text[:100],
                        "å†…å®¹": text,
                        "æ—¥æœŸ": t.get("createdAt", ""),
                        "æ¥æº": user,
                        "æ¿å—": "æ˜æ˜Ÿå…¬å¸åŠ¨æ€",
                        "é“¾æ¥": t.get("url", "")
                    })
                    count += 1
                print(f"  âœ… @{user}: {count} æ¡")
            except Exception as e:
                print(f"  âŒ @{user}: {e}")

    # ==================== TikTokï¼ˆéœ€è¦ APIï¼‰====================
    
    def fetch_tiktok(self):
        """è·å– TikTok çƒ­é—¨"""
        if not self.rapidapi_key:
            return
        
        print("\nğŸµ TikTok çƒ­é—¨...")
        
        try:
            r = requests.get("https://tiktok-api23.p.rapidapi.com/api/search/general",
                headers={
                    "x-rapidapi-key": self.rapidapi_key,
                    "x-rapidapi-host": "tiktok-api23.p.rapidapi.com"
                },
                params={"keyword": "AI", "cursor": "0"},
                timeout=30)
            data = r.json()
            
            count = 0
            for item in data.get("data", []):
                d = item.get("item", {})
                plays = d.get("stats", {}).get("playCount", 0)
                followers = d.get("authorStats", {}).get("followerCount", 1)
                
                if plays > 100000 and plays / followers > 3:
                    ts = d.get("createTime", 0)
                    if ts and datetime.fromtimestamp(ts) > self.today - timedelta(days=14):
                        author = d.get("author", {})
                        self.all_items.append({
                            "æ ‡é¢˜": d.get("desc", "")[:100],
                            "å†…å®¹": d.get("desc", ""),
                            "æ—¥æœŸ": datetime.fromtimestamp(ts).isoformat(),
                            "æ¥æº": "TikTok",
                            "æ¿å—": "TikTokçƒ­ç‚¹",
                            "é“¾æ¥": f"https://tiktok.com/@{author.get('uniqueId', '')}/video/{d.get('id', '')}"
                        })
                        count += 1
            print(f"  âœ… {count} æ¡")
        except Exception as e:
            print(f"  âŒ {e}")

    # ==================== AI å¤„ç† ====================
    
    def ai_process(self):
        """AI ç¿»è¯‘å’Œæ‘˜è¦"""
        if not self.siliconflow_key:
            print("\nâŒ æœªé…ç½® SILICONFLOW_API_KEYï¼Œæ— æ³•è¿›è¡Œ AI å¤„ç†")
            return None
        
        if not self.all_items:
            print("\nâš ï¸ æ²¡æœ‰æ•°æ®")
            return None
        
        print(f"\nğŸ¤– AI å¤„ç† ({self.model})...")
        
        prompt = f"""å¤„ç†ä»¥ä¸‹AIèµ„è®¯ï¼Œè¾“å‡ºJSONï¼š

{json.dumps(self.all_items[:50], ensure_ascii=False)}

è¦æ±‚ï¼š
1. è‹±æ–‡ç¿»è¯‘æˆä¸­æ–‡
2. é•¿å†…å®¹ç”Ÿæˆ60-80å­—æ‘˜è¦  
3. æŒ‰æ¿å—åˆ†ç»„

è¾“å‡ºæ ¼å¼ï¼š
{{"date":"{self.today_str}","categories":{{"æ–°é—»":[{{"æ ‡é¢˜":"","å†…å®¹":"","æ—¥æœŸ":"","æ¥æº":"","é“¾æ¥":""}}],"æ˜æ˜Ÿå…¬å¸åŠ¨æ€":[],"æ²¹ç®¡åšä¸»":[],"YouTubeçƒ­ç‚¹":[],"Twitterçƒ­ç‚¹":[],"TikTokçƒ­ç‚¹":[]}},"analysis":{{"summary":"ä»Šæ—¥æ‘˜è¦","trends":["è¶‹åŠ¿1"]}}}}

åªè¾“å‡ºJSONã€‚"""

        try:
            from openai import OpenAI
            client = OpenAI(
                api_key=self.siliconflow_key,
                base_url="https://api.siliconflow.cn/v1"
            )
            
            resp = client.chat.completions.create(
                model=self.model,
                messages=[{"role": "user", "content": prompt}],
                max_tokens=8000
            )
            
            content = resp.choices[0].message.content
            if "```" in content:
                content = content.split("```")[1].replace("json", "").strip()
            
            result = json.loads(content)
            
            # ä¿å­˜
            (self.data_dir / f"digest_{self.today_str}.json").write_text(
                json.dumps(result, ensure_ascii=False, indent=2), encoding="utf-8")
            (self.data_dir / "latest.json").write_text(
                json.dumps(result, ensure_ascii=False, indent=2), encoding="utf-8")
            
            total = sum(len(v) for v in result.get("categories", {}).values())
            print(f"  âœ… å®Œæˆï¼Œå…± {total} æ¡")
            return result
            
        except Exception as e:
            print(f"  âŒ {e}")
            # å¤±è´¥æ—¶ä¿å­˜åŸå§‹æ•°æ®
            fallback = {"date": self.today_str, "categories": {"åŸå§‹æ•°æ®": self.all_items}}
            (self.data_dir / "latest.json").write_text(
                json.dumps(fallback, ensure_ascii=False, indent=2), encoding="utf-8")
            return fallback

    def run(self):
        print("=" * 50)
        print(f"ğŸš€ AI èµ„è®¯èšåˆå™¨ - {self.today_str}")
        print("=" * 50)
        
        # æ•°æ®é‡‡é›†ï¼ˆæ¯ä¸ªç‹¬ç«‹ï¼Œå¤±è´¥ä¸å½±å“å…¶ä»–ï¼‰
        self.safe_fetch("RSS", self.fetch_rss)
        self.safe_fetch("YouTubeåšä¸»", self.fetch_youtube_rss)
        self.safe_fetch("YouTubeçƒ­é—¨", self.fetch_youtube_trending)
        self.safe_fetch("Twitterçƒ­é—¨", self.fetch_twitter)
        self.safe_fetch("Twitterè´¦å·", self.fetch_twitter_accounts)
        self.safe_fetch("TikTok", self.fetch_tiktok)
        
        print(f"\nğŸ“¦ å…±é‡‡é›† {len(self.all_items)} æ¡")
        
        # AI å¤„ç†
        result = self.ai_process()
        
        print("\n" + "=" * 50)
        print("âœ¨ å®Œæˆ!")
        return result


if __name__ == "__main__":
    AIDigestGenerator().run()
